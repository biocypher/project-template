# BioCypher project template
A quick way to set up a BioCypher-driven knowledge graph pipeline.

## âš™ï¸ Installation (local, for docker see below)
1. Clone this repository and rename to your project name.
```{bash}
git clone https://github.com/saezlab/biocypher-project-template.git
mv biocypher-project-template my-project
cd my-project
```
2. Make the repository your own.
```{bash}
rm -rf .git
git init
git add .
git commit -m "Initial commit"
# (you can add your remote repository here)
```
3. Install the dependencies using [Poetry](https://python-poetry.org/). (Or feel
free to use your own dependency management system. We provide a `pyproject.toml`
to define dependencies.)
```{bash}
poetry install
```
4. You are ready to go!
```{bash}
poetry shell
python create_knowledge_graph.py
```


## ğŸ›  Usage

### Structure
The project template is structured as follows:
```
.
â”‚  # Project setup
â”‚
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ pyproject.toml
â”‚
â”‚  # Docker setup
â”‚
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker
â”‚Â Â  â”œâ”€â”€ biocypher_entrypoint_patch.sh
â”‚Â Â  â”œâ”€â”€ create_table.sh
â”‚Â Â  â””â”€â”€ import.sh
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ docker-variables.env
â”‚
â”‚  # Project pipeline
â”‚
â”œâ”€â”€ create_knowledge_graph.py
â”œâ”€â”€ config
â”‚Â Â  â”œâ”€â”€ biocypher_config.yaml
â”‚Â Â  â”œâ”€â”€ biocypher_docker_config.yaml
â”‚Â Â  â””â”€â”€ schema_config.yaml
â””â”€â”€ template_package
    â””â”€â”€ adapters
        â””â”€â”€ example_adapter.py
```

The main components of the BioCypher pipeline are the
`create_knowledge_graph.py`, the configuration in the config directory, and the
adapter module in the `template_package` directory. The latter can be used to
publish your own adapters (rename the `template_package` directory to your
package name and replace the example adapter with your own). You can also use
other adapters from anywhere on GitHub, PyPI, or your local machine.

In addition, the docker setup is provided to run the pipeline (from the same
python script) in a docker container, and subsequently load the knowledge graph
into a Neo4j instance (also from a docker container). This is useful if you want
to run the pipeline on a server, or if you want to run it in a reproducible
environment.

### Running the pipeline

`python create_knowledge_graph.py` will create a knowledge graph from the
example data included in this repository (borrowed from the [BioCypher
tutorial](https://biocypher.org/tutorial.html)). To do that, it uses the
following components:

- `create_knowledge_graph.py`: the main script that orchestrates the pipeline.
It brings together the BioCypher package with the data sources. To build a 
knowledge graph, you need at least one adapter (see below). For common 
resources, there may already be an adapter available in the BioCypher package or
in a separate repository. You can also write your own adapter, should none be
available for your data.

- `example_adapter.py` (in `template_package.adapters`): a module that defines
the adapter to the data source. In this case, it is a random generator script.
If you want to create your own adapters, we recommend to use the example adapter
as a blueprint and create one python file per data source, approproately named.
You can then import the adapter in `create_knowledge_graph.py` and add it to
the pipeline. This way, you ensure that others can easily install and use your 
adapters.

- `schema_config.yaml`: a configuration file (found in the `config` directory)
that defines the schema of the knowledge graph. It is used by BioCypher to map
the data source to the knowledge representation on the basis of ontology (see
[this part of the BioCypher 
tutorial](https://biocypher.org/tutorial-ontology.html)).

- `biocypher_config.yaml`: a configuration file (found in the `config` 
directory) that defines some BioCypher parameters, such as the mode, the 
separators used, and other options. More on its use can be found in the
[Documentation](https://biocypher.org/installation.html#configuration).

## ğŸ³ Docker

This repo also contains a `docker compose` workflow to create the example
database using BioCypher and load it into a dockerised Neo4j instance
automatically. To run it, simply execute `docker compose up -d` in the root 
directory of the project. This will start up a single (detached) docker
container with a Neo4j instance that contains the knowledge graph built by
BioCypher as the DB `docker`, which you can connect to and browse at 
localhost:7474 (don't forget to switch the DB to `docker` instead of the 
standard `neo4j`). Authentication is set to `neo4j/neo4jpassword` by default
and can be modified in the `docker_variables.env` file.

By using the `BIOCYPHER_CONFIG` environment variable in the Dockerfile, the
`biocypher_docker_config.yaml` file is used instead of the 
`biocypher_config.yaml`. Everything else is the same as in the local setup. The
first container installs and runs the BioCypher pipeline, and the second
container installs and runs Neo4j. The files created by BioCypher in the first
container are copied and automatically imported into the DB in the second
container.
